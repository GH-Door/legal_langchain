import time
import json
from datetime import datetime
import pytz
from typing import Dict, List, Any
import statistics
from pathlib import Path

class ModelComparison:
    def __init__(self, version_manager=None, langsmith=None):
        self.version_manager = version_manager
        self.langsmith = langsmith
        self.results = []
        
    def compare_models(self, questions: List[str], model_configs: List[Dict], qa_chain_factory):
        """ì—¬ëŸ¬ ëª¨ë¸ë¡œ ê°™ì€ ì§ˆë¬¸ë“¤ì— ëŒ€í•´ ë‹µë³€ì„ ë¹„êµ"""
        
        if self.version_manager:
            self.version_manager.logger.info(f"=== ëª¨ë¸ ë¹„êµ ì‹œì‘ ({len(model_configs)}ê°œ ëª¨ë¸, {len(questions)}ê°œ ì§ˆë¬¸) ===")
        
        # ê° ì§ˆë¬¸ì— ëŒ€í•´ ëª¨ë“  ëª¨ë¸ì˜ ë‹µë³€ ìˆ˜ì§‘
        for i, question in enumerate(questions):
            question_results = {
                "question_id": i + 1,
                "question": question,
                "timestamp": datetime.now(pytz.timezone('Asia/Seoul')).isoformat(),
                "models": []
            }
            
            # ê° ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„±
            for model_config in model_configs:
                model_result = self._test_single_model(question, model_config, qa_chain_factory)
                question_results["models"].append(model_result)
                
                # ì§„í–‰ìƒí™© ì¶œë ¥
                print(f"ì§ˆë¬¸ {i+1}/{len(questions)}: {model_config['name']} ì™„ë£Œ")
            
            self.results.append(question_results)
            
        # ë¹„êµ ê²°ê³¼ ë¶„ì„
        analysis = self._analyze_results()
        
        # ê²°ê³¼ ì €ì¥
        self._save_results(analysis)
        
        return analysis
    
    def _test_single_model(self, question: str, model_config: Dict, qa_chain_factory) -> Dict:
        """ë‹¨ì¼ ëª¨ë¸ë¡œ ì§ˆë¬¸ì— ë‹µë³€"""
        
        start_time = time.time()
        
        try:
            # QA ì²´ì¸ ìƒì„±
            qa_chain = qa_chain_factory(model_config)
            
            # ë‹µë³€ ìƒì„±
            response = qa_chain.invoke(question)
            
            # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
            execution_time = time.time() - start_time
            
            result = {
                "model_name": model_config['name'],
                "model_provider": model_config['provider'],
                "model_id": model_config['model_name'],
                "temperature": model_config.get('temperature', 0.7),
                "response": response,
                "execution_time": execution_time,
                "success": True,
                "error": None,
                "response_length": len(str(response)),
                "tokens_estimated": self._estimate_tokens(str(response))
            }
            
            # ë¡œê¹…
            if self.version_manager:
                self.version_manager.logger.info(f"[{model_config['name']}] ì‹¤í–‰ì‹œê°„: {execution_time:.2f}ì´ˆ")
                
        except Exception as e:
            execution_time = time.time() - start_time
            result = {
                "model_name": model_config['name'],
                "model_provider": model_config['provider'],
                "model_id": model_config['model_name'],
                "temperature": model_config.get('temperature', 0.7),
                "response": None,
                "execution_time": execution_time,
                "success": False,
                "error": str(e),
                "response_length": 0,
                "tokens_estimated": 0
            }
            
            if self.version_manager:
                self.version_manager.logger.error(f"[{model_config['name']}] ì˜¤ë¥˜: {e}")
        
        return result
    
    def _estimate_tokens(self, text: str) -> int:
        """í† í° ìˆ˜ ì¶”ì • (ëŒ€ëµì )"""
        # ê°„ë‹¨í•œ ì¶”ì •: ë‹¨ì–´ ìˆ˜ì˜ 1.3ë°° ì •ë„
        words = len(text.split())
        return int(words * 1.3)
    
    def _analyze_results(self) -> Dict:
        """ê²°ê³¼ ë¶„ì„"""
        
        analysis = {
            "summary": {
                "total_questions": len(self.results),
                "total_models": len(self.results[0]["models"]) if self.results else 0,
                "timestamp": datetime.now(pytz.timezone('Asia/Seoul')).isoformat()
            },
            "model_performance": {},
            "detailed_comparison": self.results
        }
        
        if not self.results:
            return analysis
        
        # ëª¨ë¸ë³„ ì„±ëŠ¥ í†µê³„
        model_names = [model["model_name"] for model in self.results[0]["models"]]
        
        for model_name in model_names:
            model_data = []
            success_count = 0
            total_time = 0
            total_length = 0
            total_tokens = 0
            
            # ê° ì§ˆë¬¸ì˜ í•´ë‹¹ ëª¨ë¸ ê²°ê³¼ ìˆ˜ì§‘
            for question_result in self.results:
                for model_result in question_result["models"]:
                    if model_result["model_name"] == model_name:
                        model_data.append(model_result)
                        if model_result["success"]:
                            success_count += 1
                            total_time += model_result["execution_time"]
                            total_length += model_result["response_length"]
                            total_tokens += model_result["tokens_estimated"]
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
            analysis["model_performance"][model_name] = {
                "success_rate": success_count / len(model_data) if model_data else 0,
                "average_response_time": total_time / success_count if success_count > 0 else 0,
                "average_response_length": total_length / success_count if success_count > 0 else 0,
                "average_tokens": total_tokens / success_count if success_count > 0 else 0,
                "total_questions": len(model_data),
                "successful_responses": success_count,
                "failed_responses": len(model_data) - success_count
            }
        
        return analysis
    
    def _save_results(self, analysis: Dict):
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
        results_dir = Path("results")
        results_dir.mkdir(exist_ok=True)
        
        # í•œêµ­ì‹œê° ê¸°ì¤€ íŒŒì¼ëª… ìƒì„±
        kst = pytz.timezone('Asia/Seoul')
        now = datetime.now(kst)
        timestamp = now.strftime("%m%d%H%M")
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        json_file = results_dir / f"model_comparison_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        
        # ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ ë¦¬í¬íŠ¸ ìƒì„±
        report_file = results_dir / f"model_comparison_report_{timestamp}.md"
        self._generate_report(analysis, report_file)
        
        if self.version_manager:
            self.version_manager.logger.info(f"ë¹„êµ ê²°ê³¼ ì €ì¥: {json_file}")
            self.version_manager.logger.info(f"ë¦¬í¬íŠ¸ ìƒì„±: {report_file}")
    
    def _generate_report(self, analysis: Dict, report_file: Path):
        """ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# ğŸ¤– LLM ëª¨ë¸ ë¹„êµ ë¦¬í¬íŠ¸\n\n")
            
            # ìš”ì•½ ì •ë³´
            summary = analysis["summary"]
            f.write(f"**ìƒì„± ì‹œê°„**: {summary['timestamp']}\n")
            f.write(f"**ì´ ì§ˆë¬¸ ìˆ˜**: {summary['total_questions']}\n")
            f.write(f"**ë¹„êµ ëª¨ë¸ ìˆ˜**: {summary['total_models']}\n\n")
            
            # ì„±ëŠ¥ ë¹„êµí‘œ
            f.write("## ğŸ“Š ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ\n\n")
            f.write("| ëª¨ë¸ | ì„±ê³µë¥  | í‰ê·  ì‘ë‹µì‹œê°„ | í‰ê·  ì‘ë‹µê¸¸ì´ | í‰ê·  í† í°ìˆ˜ |\n")
            f.write("|------|---------|---------------|---------------|-------------|\n")
            
            for model_name, perf in analysis["model_performance"].items():
                f.write(f"| {model_name} | {perf['success_rate']:.1%} | {perf['average_response_time']:.2f}s | {perf['average_response_length']:.0f} | {perf['average_tokens']:.0f} |\n")
            
            f.write("\n## ğŸ“ ìƒì„¸ ì§ˆë¬¸ë³„ ë¹„êµ\n\n")
            
            # ê° ì§ˆë¬¸ë³„ ìƒì„¸ ê²°ê³¼
            for i, question_result in enumerate(analysis["detailed_comparison"]):
                f.write(f"### ì§ˆë¬¸ {i+1}\n")
                f.write(f"**ì§ˆë¬¸**: {question_result['question']}\n\n")
                
                for model_result in question_result["models"]:
                    f.write(f"#### {model_result['model_name']}\n")
                    if model_result["success"]:
                        f.write(f"**ì‘ë‹µ**: {model_result['response']}\n")
                        f.write(f"**ì‹¤í–‰ì‹œê°„**: {model_result['execution_time']:.2f}ì´ˆ\n")
                        f.write(f"**ì‘ë‹µê¸¸ì´**: {model_result['response_length']}ì\n\n")
                    else:
                        f.write(f"**ì˜¤ë¥˜**: {model_result['error']}\n\n")
                
                f.write("---\n\n")
        
        print(f"\nğŸ“Š ìƒì„¸ ë¹„êµ ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {report_file}")