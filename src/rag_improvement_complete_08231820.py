#!/usr/bin/env python3
"""
RAG ì„±ëŠ¥ ê°œì„  ë¹„êµ ì™„ë²½ ì‹œìŠ¤í…œ v08231820
LangSmith ì¶”ì , Streamlit/Gradio ì›¹ ì¸í„°í˜ì´ìŠ¤ í†µí•©
ìˆœìˆ˜ LLM vs RAG ì ìš© ëª¨ë¸ì˜ ì„±ëŠ¥ ê°œì„ ë„ë¥¼ ì¸¡ì •í•˜ê³  ë¹„êµ ë¶„ì„

ì™„ë²½ í†µí•© ê¸°ëŠ¥:
- LangSmith ì „ì²´ ì¶”ì  ì‹œìŠ¤í…œ
- Streamlit ì „ë¬¸ì  ì¸í„°í˜ì´ìŠ¤
- Gradio ë‹¨ìˆœ ì¸í„°í˜ì´ìŠ¤
- ì‹¤ì‹œê°„ ê²°ê³¼ ì‹œê°í™”
- JSON/Markdown/CSV ë‹¤ì¤‘ ì¶œë ¥
"""

import os
import json
import time
import re
import csv
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv
from omegaconf import OmegaConf

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
import sys
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from src.utils.version_manager import VersionManager
from src.utils.langsmith_simple import LangSmithSimple
from src.utils.path_utils import ensure_directory_exists

# OpenAI ë° Anthropic ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

# LangSmith ì¶”ì 
try:
    from langsmith import traceable
    LANGSMITH_AVAILABLE = True
except ImportError:
    LANGSMITH_AVAILABLE = False
    def traceable(func):
        return func


class LawCaseLoader:
    """ë²•ë¥  íŒë¡€ ë¡œë” ë° ê²€ìƒ‰ê¸° (LangSmith ì¶”ì  í¬í•¨)"""
    
    def __init__(self, law_data_dir: str = "data/law"):
        self.law_data_dir = Path(law_data_dir)
        self.cases = []
        
    @traceable(name="load_legal_cases")
    def load_cases(self):
        """ëª¨ë“  íŒë¡€ ë¡œë“œ - LangSmith ì¶”ì """
        if not self.law_data_dir.exists():
            raise FileNotFoundError(f"ë²•ë¥  ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.law_data_dir}")
        
        json_files = list(self.law_data_dir.glob("*.json"))
        
        for json_file in json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    case_data = json.load(f)
                
                self.cases.append({
                    'case_number': case_data.get('ì‚¬ê±´ë²ˆí˜¸', ''),
                    'case_name': case_data.get('ì‚¬ê±´ëª…', ''),
                    'court': case_data.get('ë²•ì›ëª…', ''),
                    'date': case_data.get('ì„ ê³ ì¼ì', ''),
                    'case_type': case_data.get('ì‚¬ê±´ì¢…ë¥˜ëª…', ''),
                    'summary': case_data.get('íŒì‹œì‚¬í•­', ''),
                    'decision': case_data.get('íŒê²°ìš”ì§€', ''),
                    'references': case_data.get('ì°¸ì¡°ì¡°ë¬¸', ''),
                    'content': case_data.get('íŒë¡€ë‚´ìš©', ''),
                    'full_text': self._format_case_text(case_data)
                })
                
            except Exception as e:
                print(f"íŒë¡€ ë¡œë“œ ì˜¤ë¥˜ {json_file}: {e}")
                continue
        
        print(f"ì´ {len(self.cases)}ê°œ íŒë¡€ ë¡œë“œ ì™„ë£Œ")
        return self.cases
    
    def _format_case_text(self, case_data: dict) -> str:
        """íŒë¡€ë¥¼ RAGìš© í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
        return f"""
ì‚¬ê±´ë²ˆí˜¸: {case_data.get('ì‚¬ê±´ë²ˆí˜¸', '') or 'N/A'}
ì‚¬ê±´ëª…: {case_data.get('ì‚¬ê±´ëª…', '') or 'N/A'}
ë²•ì›: {case_data.get('ë²•ì›ëª…', '') or 'N/A'} ({case_data.get('ì„ ê³ ì¼ì', '') or 'N/A'})

íŒì‹œì‚¬í•­:
{case_data.get('íŒì‹œì‚¬í•­', '') or 'ì •ë³´ ì—†ìŒ'}

íŒê²°ìš”ì§€:
{case_data.get('íŒê²°ìš”ì§€', '') or 'ì •ë³´ ì—†ìŒ'}

ì°¸ì¡°ì¡°ë¬¸:
{case_data.get('ì°¸ì¡°ì¡°ë¬¸', '') or 'ì •ë³´ ì—†ìŒ'}
"""
    
    @traceable(name="search_relevant_cases")
    def search_relevant_cases(self, question: str, top_k: int = 3) -> list:
        """ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ íŒë¡€ ê²€ìƒ‰ - LangSmith ì¶”ì """
        if not self.cases:
            return []
        
        question_keywords = question.lower().split()
        scored_cases = []
        
        for case in self.cases:
            # ê²€ìƒ‰ ëŒ€ìƒ í…ìŠ¤íŠ¸ (íŒì‹œì‚¬í•­ + íŒê²°ìš”ì§€ + ì‚¬ê±´ëª…)
            search_text = (
                (case['summary'] or '') + ' ' + 
                (case['decision'] or '') + ' ' + 
                (case['case_name'] or '')
            ).lower()
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
            score = 0
            for keyword in question_keywords:
                if len(keyword) > 1:  # í•œ ê¸€ì í‚¤ì›Œë“œ ì œì™¸
                    count = search_text.count(keyword)
                    score += count * len(keyword)  # ê¸´ í‚¤ì›Œë“œì— ê°€ì¤‘ì¹˜
            
            if score > 0:
                scored_cases.append((case, score))
        
        # ì ìˆ˜ìˆœ ì •ë ¬í•˜ì—¬ ìƒìœ„ kê°œ ë°˜í™˜
        scored_cases.sort(key=lambda x: x[1], reverse=True)
        selected_cases = [case[0] for case in scored_cases[:top_k]]
        
        return selected_cases


class RAGImprovementComparator:
    """RAG ì„±ëŠ¥ ê°œì„  ë¹„êµ ë¶„ì„ê¸° (ì™„ë²½ ë²„ì „)"""
    
    def __init__(self, version_manager: VersionManager, langsmith_manager=None):
        self.version_manager = version_manager
        self.langsmith_manager = langsmith_manager
        self.openai_client = None
        self.anthropic_client = None
        self.case_loader = LawCaseLoader()
        
        # API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        # load_dotenv()  # main()ì—ì„œ ì´ë¯¸ í˜¸ì¶œë¨
        
        if OPENAI_AVAILABLE and os.getenv("OPENAI_API_KEY"):
            self.openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            
        if ANTHROPIC_AVAILABLE and os.getenv("ANTHROPIC_API_KEY"):
            self.anthropic_client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
    
    @traceable(name="pure_llm_response")
    def get_pure_llm_response(self, model_name: str, question: str, temperature: float = 0.1) -> dict:
        """ìˆœìˆ˜ LLM ì‘ë‹µ (RAG ì—†ìŒ) - LangSmith ì¶”ì """
        start_time = time.time()
        
        system_prompt = """ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ì˜ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•´ ë²•ë¥  ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. 
ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì ì¸ ë²•ì¡°ë¬¸ì´ë‚˜ íŒë¡€ë¥¼ ì–¸ê¸‰í•˜ì—¬ ë‹µë³€í•˜ë˜, í™•ì‹¤í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì¼ë°˜ì ì¸ ë²•ë¥  ì›ì¹™ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”."""
        
        try:
            if model_name.lower().startswith('gpt') and self.openai_client:
                response = self.openai_client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": question}
                    ],
                    temperature=temperature,
                    max_tokens=1000
                )
                answer = response.choices[0].message.content
                
            elif model_name.lower().startswith('claude') and self.anthropic_client:
                response = self.anthropic_client.messages.create(
                    model="claude-3-5-haiku-20241022",
                    max_tokens=1000,
                    temperature=temperature,
                    system=system_prompt,
                    messages=[
                        {"role": "user", "content": question}
                    ]
                )
                answer = response.content[0].text
                
            else:
                return {
                    'success': False,
                    'answer': f"{model_name} API í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                    'response_time': 0,
                    'model': model_name,
                    'type': 'pure'
                }
            
            response_time = time.time() - start_time
            
            return {
                'success': True,
                'answer': answer,
                'response_time': response_time,
                'model': model_name,
                'type': 'pure',
                'case_count': 0,
                'cases_used': [],
                'answer_length': len(answer),
                'word_count': len(answer.split())
            }
            
        except Exception as e:
            response_time = time.time() - start_time
            return {
                'success': False,
                'answer': f"{model_name} ì˜¤ë¥˜: {str(e)}",
                'response_time': response_time,
                'model': model_name,
                'type': 'pure'
            }
    
    @traceable(name="rag_llm_response")  
    def get_rag_response(self, model_name: str, question: str, temperature: float = 0.1) -> dict:
        """RAG ì ìš© LLM ì‘ë‹µ - LangSmith ì¶”ì """
        start_time = time.time()
        
        # ê´€ë ¨ íŒë¡€ ê²€ìƒ‰
        relevant_cases = self.case_loader.search_relevant_cases(question, top_k=3)
        
        # RAG ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        if relevant_cases:
            context = "ê´€ë ¨ íŒë¡€ ì •ë³´:\n\n"
            for i, case in enumerate(relevant_cases, 1):
                context += f"{i}. {case['full_text']}\n{'='*50}\n"
        else:
            context = "ê´€ë ¨ íŒë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        system_prompt = """ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ì˜ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ íŒë¡€ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. 
ë‹µë³€í•  ë•ŒëŠ” ë°˜ë“œì‹œ ê´€ë ¨ íŒë¡€ì˜ ì‚¬ê±´ë²ˆí˜¸ë¥¼ ì¸ìš©í•˜ê³ , íŒë¡€ì˜ ë‚´ìš©ì„ ê·¼ê±°ë¡œ ì œì‹œí•˜ì—¬ ì‹ ë¢°ì„± ìˆëŠ” ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”."""
        
        user_prompt = f"""
ì§ˆë¬¸: {question}

{context}

ìœ„ì˜ íŒë¡€ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ êµ¬ì²´ì ì´ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""
        
        try:
            if model_name.lower().startswith('gpt') and self.openai_client:
                response = self.openai_client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=temperature,
                    max_tokens=1200
                )
                answer = response.choices[0].message.content
                
            elif model_name.lower().startswith('claude') and self.anthropic_client:
                response = self.anthropic_client.messages.create(
                    model="claude-3-5-haiku-20241022",
                    max_tokens=1200,
                    temperature=temperature,
                    system=system_prompt,
                    messages=[
                        {"role": "user", "content": user_prompt}
                    ]
                )
                answer = response.content[0].text
                
            else:
                return {
                    'success': False,
                    'answer': f"{model_name} API í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                    'response_time': 0,
                    'model': model_name,
                    'type': 'rag'
                }
            
            response_time = time.time() - start_time
            
            return {
                'success': True,
                'answer': answer,
                'response_time': response_time,
                'model': model_name,
                'type': 'rag',
                'case_count': len(relevant_cases),
                'cases_used': [case['case_number'] for case in relevant_cases],
                'answer_length': len(answer),
                'word_count': len(answer.split()),
                'context_length': len(context)
            }
            
        except Exception as e:
            response_time = time.time() - start_time
            return {
                'success': False,
                'answer': f"{model_name} RAG ì˜¤ë¥˜: {str(e)}",
                'response_time': response_time,
                'model': model_name,
                'type': 'rag'
            }
    
    def analyze_improvement(self, pure_result: dict, rag_result: dict) -> dict:
        """RAG ì ìš©ìœ¼ë¡œ ì¸í•œ ê°œì„ ë„ ë¶„ì„ (ìƒì„¸ ë©”íŠ¸ë¦­ í¬í•¨)"""
        
        if not (pure_result['success'] and rag_result['success']):
            return {
                'overall_score': 0,
                'specificity_improvement': 0,
                'evidence_improvement': 0,
                'length_change': 0,
                'word_count_change': 0,
                'response_time_change': 0,
                'case_citation_count': 0,
                'legal_keyword_density': 0,
                'analysis': "ë¶„ì„ ë¶ˆê°€ (API ì˜¤ë¥˜)"
            }
        
        pure_answer = pure_result['answer']
        rag_answer = rag_result['answer']
        
        # 1. êµ¬ì²´ì„± ê°œì„  (ì‚¬ê±´ë²ˆí˜¸ ì¸ìš© ì—¬ë¶€)
        pure_case_refs = len(re.findall(r'\d{4}[ê°€-í£]+\d+', pure_answer))
        rag_case_refs = len(re.findall(r'\d{4}[ê°€-í£]+\d+', rag_answer))
        specificity_improvement = rag_case_refs - pure_case_refs
        
        # 2. ê·¼ê±° ì œì‹œ ê°œì„  (ë²•ì¡°ë¬¸, íŒë¡€ í‚¤ì›Œë“œ)
        evidence_keywords = ['íŒë¡€', 'íŒê²°', 'ëŒ€ë²•ì›', 'ë²•ì›', 'ì¡°ë¬¸', 'ë²•ë¥ ', 'ê·œì •', 'íŒì‹œì‚¬í•­', 'íŒê²°ìš”ì§€']
        pure_evidence = sum(pure_answer.lower().count(kw) for kw in evidence_keywords)
        rag_evidence = sum(rag_answer.lower().count(kw) for kw in evidence_keywords)
        evidence_improvement = rag_evidence - pure_evidence
        
        # 3. ë‹µë³€ ê¸¸ì´ ë° ë‹¨ì–´ ìˆ˜ ë³€í™”
        length_change = rag_result.get('answer_length', len(rag_answer)) - pure_result.get('answer_length', len(pure_answer))
        word_count_change = rag_result.get('word_count', len(rag_answer.split())) - pure_result.get('word_count', len(pure_answer.split()))
        
        # 4. ì‘ë‹µ ì‹œê°„ ë³€í™”
        response_time_change = rag_result['response_time'] - pure_result['response_time']
        
        # 5. ë²•ë¥  í‚¤ì›Œë“œ ë°€ë„ (ë‹µë³€ ê¸¸ì´ ëŒ€ë¹„)
        legal_keyword_density = rag_evidence / max(len(rag_answer), 1) * 1000  # 1000ê¸€ìë‹¹ í‚¤ì›Œë“œ ìˆ˜
        
        # 6. ì „ì²´ì  ê°œì„  ì ìˆ˜ ê³„ì‚° (0-100ì )
        overall_score = min(100, max(0, 
            (specificity_improvement * 20) +     # ì‚¬ê±´ë²ˆí˜¸ ì¸ìš©ë‹¹ 20ì 
            (evidence_improvement * 5) +         # ë²•ë¥  í‚¤ì›Œë“œë‹¹ 5ì   
            (min(length_change, 500) / 10) +     # ê¸¸ì´ ì¦ê°€ë¶„ ìµœëŒ€ 50ì 
            (rag_result.get('case_count', 0) * 5)  # ì‚¬ìš©ëœ íŒë¡€ë‹¹ 5ì 
        ))
        
        # 7. ë¶„ì„ ìš”ì•½
        analysis_parts = []
        
        if specificity_improvement > 0:
            analysis_parts.append(f"êµ¬ì²´ì„± í–¥ìƒ: {specificity_improvement}ê°œ ì‚¬ê±´ë²ˆí˜¸ ì¶”ê°€ ì¸ìš©")
        
        if evidence_improvement > 0:
            analysis_parts.append(f"ê·¼ê±° ê°•í™”: {evidence_improvement}ê°œ ë²•ë¥  í‚¤ì›Œë“œ ì¶”ê°€")
        
        if length_change > 0:
            analysis_parts.append(f"ì •ë³´ëŸ‰ ì¦ê°€: {length_change:,}ê¸€ì ì¶”ê°€")
            
        if rag_result.get('case_count', 0) > 0:
            analysis_parts.append(f"íŒë¡€ í™œìš©: {rag_result.get('case_count', 0)}ê±´ ì°¸ì¡°")
        
        if not analysis_parts:
            analysis_parts.append("ê°œì„  íš¨ê³¼ ë¯¸ë¯¸")
        
        analysis = " / ".join(analysis_parts)
        
        return {
            'overall_score': round(overall_score, 1),
            'specificity_improvement': specificity_improvement,
            'evidence_improvement': evidence_improvement, 
            'length_change': length_change,
            'word_count_change': word_count_change,
            'response_time_change': round(response_time_change, 2),
            'case_citation_count': rag_case_refs,
            'legal_keyword_density': round(legal_keyword_density, 2),
            'analysis': analysis
        }
    
    @traceable(name="compare_models_complete")
    def compare_models(self, questions: list, temperature: float = 0.1, progress_callback=None) -> dict:
        """ëª¨ë¸ë³„ RAG ì„±ëŠ¥ ê°œì„  ë¹„êµ (ì™„ë²½ ë²„ì „)"""
        
        # íŒë¡€ ë¡œë“œ
        self.case_loader.load_cases()
        
        results = {
            'timestamp': datetime.now().isoformat(),
            'version': 'v08231820',
            'questions': {},
            'summary': {},
            'metadata': {
                'total_cases': len(self.case_loader.cases),
                'temperature': temperature,
                'models_tested': ['GPT-4o', 'Claude-3.5']
            }
        }
        
        models = ['GPT-4o', 'Claude-3.5']
        total_steps = len(questions) * len(models) * 2  # 2 = pure + rag
        current_step = 0
        
        for q_idx, question in enumerate(questions, 1):
            question_id = f"q{q_idx}"
            results['questions'][question_id] = {
                'question': question,
                'responses': {},
                'improvements': {},
                'metrics': {}
            }
            
            print(f"\n{'='*60}")
            print(f"ì§ˆë¬¸ {q_idx}: {question}")
            print('='*60)
            
            for model in models:
                print(f"\n--- {model} ë¶„ì„ ì¤‘ ---")
                
                # ìˆœìˆ˜ LLM ì‘ë‹µ
                print("ìˆœìˆ˜ LLM ë‹µë³€ ìƒì„± ì¤‘...")
                pure_result = self.get_pure_llm_response(model, question, temperature)
                current_step += 1
                
                if progress_callback:
                    progress_callback(current_step / total_steps)
                
                # RAG ì ìš© ì‘ë‹µ  
                print("RAG ì ìš© ë‹µë³€ ìƒì„± ì¤‘...")
                rag_result = self.get_rag_response(model, question, temperature)
                current_step += 1
                
                if progress_callback:
                    progress_callback(current_step / total_steps)
                
                # ê²°ê³¼ ì €ì¥
                results['questions'][question_id]['responses'][model] = {
                    'pure': pure_result,
                    'rag': rag_result
                }
                
                # ê°œì„ ë„ ë¶„ì„
                improvement = self.analyze_improvement(pure_result, rag_result)
                results['questions'][question_id]['improvements'][model] = improvement
                
                # ì¶”ê°€ ë©”íŠ¸ë¦­
                results['questions'][question_id]['metrics'][model] = {
                    'pure_answer_length': pure_result.get('answer_length', 0),
                    'rag_answer_length': rag_result.get('answer_length', 0),
                    'pure_response_time': pure_result.get('response_time', 0),
                    'rag_response_time': rag_result.get('response_time', 0),
                    'cases_used_count': rag_result.get('case_count', 0),
                    'improvement_percentage': improvement['overall_score']
                }
                
                # ì§„í–‰ ìƒí™© ì¶œë ¥
                if pure_result['success'] and rag_result['success']:
                    print(f"âœ… {model} ì™„ë£Œ - ê°œì„  ì ìˆ˜: {improvement['overall_score']:.1f}/100")
                    print(f"   ìˆœìˆ˜ ë‹µë³€: {pure_result.get('answer_length', 0)}ê¸€ì ({pure_result['response_time']:.2f}ì´ˆ)")
                    print(f"   RAG ë‹µë³€: {rag_result.get('answer_length', 0)}ê¸€ì ({rag_result['response_time']:.2f}ì´ˆ)")
                    print(f"   ì‚¬ìš© íŒë¡€: {rag_result.get('case_count', 0)}ê±´")
                else:
                    print(f"âŒ {model} ì˜¤ë¥˜ ë°œìƒ")
        
        # ì „ì²´ ìš”ì•½ í†µê³„ ìƒì„±
        results['summary'] = self._generate_summary(results)
        
        return results
    
    def _generate_summary(self, results: dict) -> dict:
        """ì „ì²´ ë¶„ì„ ê²°ê³¼ ìš”ì•½ (ìƒì„¸ í†µê³„ í¬í•¨)"""
        summary = {
            'total_questions': len(results['questions']),
            'model_averages': {},
            'best_improvements': {},
            'performance_comparison': {},
            'efficiency_metrics': {}
        }
        
        models = ['GPT-4o', 'Claude-3.5']
        
        for model in models:
            scores = []
            time_changes = []
            length_changes = []
            case_counts = []
            
            for q_data in results['questions'].values():
                if model in q_data['improvements']:
                    improvement = q_data['improvements'][model]
                    metrics = q_data['metrics'][model]
                    
                    scores.append(improvement['overall_score'])
                    time_changes.append(improvement['response_time_change'])
                    length_changes.append(improvement['length_change'])
                    case_counts.append(metrics['cases_used_count'])
            
            if scores:
                summary['model_averages'][model] = {
                    'avg_improvement_score': round(sum(scores) / len(scores), 1),
                    'avg_time_increase': round(sum(time_changes) / len(time_changes), 2),
                    'avg_length_increase': round(sum(length_changes) / len(length_changes), 1),
                    'avg_cases_used': round(sum(case_counts) / len(case_counts), 1),
                    'questions_analyzed': len(scores),
                    'best_score': max(scores),
                    'worst_score': min(scores)
                }
        
        # ëª¨ë¸ê°„ ë¹„êµ
        if len(summary['model_averages']) >= 2:
            gpt_avg = summary['model_averages'].get('GPT-4o', {})
            claude_avg = summary['model_averages'].get('Claude-3.5', {})
            
            summary['performance_comparison'] = {
                'better_improvement': 'GPT-4o' if gpt_avg.get('avg_improvement_score', 0) > claude_avg.get('avg_improvement_score', 0) else 'Claude-3.5',
                'faster_processing': 'GPT-4o' if gpt_avg.get('avg_time_increase', 0) < claude_avg.get('avg_time_increase', 0) else 'Claude-3.5',
                'score_difference': abs(gpt_avg.get('avg_improvement_score', 0) - claude_avg.get('avg_improvement_score', 0))
            }
        
        return summary


def save_results_multiple_formats(results: dict, output_dir: Path, timestamp: str):
    """ê²°ê³¼ë¥¼ ì—¬ëŸ¬ í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
    
    # JSON ê²°ê³¼ ì €ì¥
    json_path = output_dir / f"rag_improvement_complete_{timestamp}.json"
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # CSV ìš”ì•½ ì €ì¥
    csv_path = output_dir / f"rag_improvement_summary_{timestamp}.csv"
    with open(csv_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        
        # í—¤ë”
        writer.writerow([
            'ì§ˆë¬¸ID', 'ì§ˆë¬¸ë‚´ìš©', 'ëª¨ë¸', 'ìˆœìˆ˜ì ìˆ˜', 'RAGì ìˆ˜', 'ê°œì„ ì ìˆ˜', 
            'ìˆœìˆ˜ì‘ë‹µì‹œê°„', 'RAGì‘ë‹µì‹œê°„', 'ì‹œê°„ë³€í™”', 'ì‚¬ìš©íŒë¡€ìˆ˜', 'ë¶„ì„ê²°ê³¼'
        ])
        
        # ë°ì´í„°
        for q_id, q_data in results.get('questions', {}).items():
            for model in ['GPT-4o', 'Claude-3.5']:
                if model in q_data.get('improvements', {}):
                    improvement = q_data['improvements'][model]
                    responses = q_data['responses'][model]
                    
                    writer.writerow([
                        q_id,
                        q_data['question'][:50] + '...',
                        model,
                        'N/A',  # ìˆœìˆ˜ì ìˆ˜ëŠ” ë³„ë„ ê³„ì‚° í•„ìš”
                        'N/A',  # RAGì ìˆ˜ëŠ” ë³„ë„ ê³„ì‚° í•„ìš”
                        improvement['overall_score'],
                        responses['pure']['response_time'],
                        responses['rag']['response_time'],
                        improvement['response_time_change'],
                        responses['rag'].get('case_count', 0),
                        improvement['analysis'][:100] + '...'
                    ])
    
    # ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±
    report_path = output_dir / f"rag_improvement_complete_report_{timestamp}.md"
    generate_complete_markdown_report(results, report_path)
    
    return json_path, csv_path, report_path


def generate_complete_markdown_report(results: dict, report_path: Path):
    """ì™„ì „í•œ ë§ˆí¬ë‹¤ìš´ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# RAG ì„±ëŠ¥ ê°œì„  ë¹„êµ ì™„ë²½ ë¶„ì„ ë³´ê³ ì„œ\n\n")
        f.write(f"**ë²„ì „**: {results.get('version', 'N/A')}\n")
        f.write(f"**ìƒì„±ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"**ë¶„ì„ íŒë¡€ ìˆ˜**: {results.get('metadata', {}).get('total_cases', 0)}ê±´\n\n")
        
        # ìš”ì•½ ì •ë³´
        summary = results.get('summary', {})
        f.write("## ğŸ“Š ì „ì²´ ë¶„ì„ ìš”ì•½\n\n")
        f.write(f"- **ë¶„ì„ ì§ˆë¬¸ ìˆ˜**: {summary.get('total_questions', 0)}ê°œ\n")
        
        if 'model_averages' in summary:
            for model, avg_data in summary['model_averages'].items():
                f.write(f"\n### {model} ì„±ëŠ¥ ìš”ì•½\n")
                f.write(f"- **í‰ê·  ê°œì„  ì ìˆ˜**: {avg_data.get('avg_improvement_score', 0):.1f}/100\n")
                f.write(f"- **í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì¦ê°€**: {avg_data.get('avg_time_increase', 0):+.2f}ì´ˆ\n")
                f.write(f"- **í‰ê·  ë‹µë³€ ê¸¸ì´ ì¦ê°€**: {avg_data.get('avg_length_increase', 0):+.0f}ê¸€ì\n")
                f.write(f"- **í‰ê·  ì‚¬ìš© íŒë¡€**: {avg_data.get('avg_cases_used', 0):.1f}ê±´\n")
                f.write(f"- **ìµœê³ /ìµœì € ì ìˆ˜**: {avg_data.get('best_score', 0):.1f} / {avg_data.get('worst_score', 0):.1f}\n")
        
        # ëª¨ë¸ ë¹„êµ
        if 'performance_comparison' in summary:
            comp = summary['performance_comparison']
            f.write(f"\n## ğŸ† ëª¨ë¸ê°„ ì„±ëŠ¥ ë¹„êµ\n\n")
            f.write(f"- **ë” ë‚˜ì€ ê°œì„  íš¨ê³¼**: {comp.get('better_improvement', 'N/A')}\n")
            f.write(f"- **ë” ë¹ ë¥¸ ì²˜ë¦¬**: {comp.get('faster_processing', 'N/A')}\n")
            f.write(f"- **ì ìˆ˜ ì°¨ì´**: {comp.get('score_difference', 0):.1f}ì \n\n")
        
        f.write("## ğŸ” ì§ˆë¬¸ë³„ ìƒì„¸ ë¶„ì„\n\n")
        
        # ì§ˆë¬¸ë³„ ê²°ê³¼
        for q_id, q_data in results.get('questions', {}).items():
            f.write(f"### {q_id.upper()}. {q_data['question']}\n\n")
            
            for model in ['GPT-4o', 'Claude-3.5']:
                if model in q_data.get('improvements', {}):
                    improvement = q_data['improvements'][model]
                    responses = q_data['responses'][model]
                    metrics = q_data.get('metrics', {}).get(model, {})
                    
                    f.write(f"#### {model} ìƒì„¸ ë¶„ì„\n")
                    f.write(f"- **ê°œì„  ì ìˆ˜**: {improvement['overall_score']:.1f}/100\n")
                    f.write(f"- **ë¶„ì„ ê²°ê³¼**: {improvement['analysis']}\n")
                    f.write(f"- **ì‘ë‹µ ì‹œê°„ ë³€í™”**: {improvement['response_time_change']:+.2f}ì´ˆ\n")
                    f.write(f"- **ë‹µë³€ ê¸¸ì´ ë³€í™”**: {improvement['length_change']:+d}ê¸€ì\n")
                    f.write(f"- **ë‹¨ì–´ ìˆ˜ ë³€í™”**: {improvement['word_count_change']:+d}ê°œ\n")
                    f.write(f"- **ì‚¬ìš©ëœ íŒë¡€**: {responses['rag'].get('case_count', 0)}ê±´\n")
                    f.write(f"- **ë²•ë¥  í‚¤ì›Œë“œ ë°€ë„**: {improvement['legal_keyword_density']:.2f}/1000ê¸€ì\n\n")
                    
                    # ìˆœìˆ˜ vs RAG ë‹µë³€ ë¹„êµ (ì¶•ì•½)
                    f.write(f"**ìˆœìˆ˜ {model} ë‹µë³€** ({metrics.get('pure_answer_length', 0)}ê¸€ì, {responses['pure']['response_time']:.2f}ì´ˆ):\n")
                    f.write(f"```\n{responses['pure']['answer'][:200]}{'...' if len(responses['pure']['answer']) > 200 else ''}\n```\n\n")
                    
                    f.write(f"**RAG ì ìš© {model} ë‹µë³€** ({metrics.get('rag_answer_length', 0)}ê¸€ì, {responses['rag']['response_time']:.2f}ì´ˆ):\n")
                    f.write(f"```\n{responses['rag']['answer'][:200]}{'...' if len(responses['rag']['answer']) > 200 else ''}\n```\n\n")
                    
                    if responses['rag'].get('cases_used'):
                        f.write(f"**ì°¸ì¡°ëœ íŒë¡€**: {', '.join(responses['rag']['cases_used'])}\n\n")
                    
                    f.write("---\n\n")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ì™„ë²½ ë²„ì „)"""
    
    print("ğŸš€ RAG ì„±ëŠ¥ ê°œì„  ë¹„êµ ì™„ë²½ ì‹œìŠ¤í…œ v08231820 ì‹œì‘")
    
    # í™˜ê²½ ë³€ìˆ˜ ë¨¼ì € ë¡œë“œ
    load_dotenv()
    
    # ë²„ì „ ê´€ë¦¬ì ì´ˆê¸°í™”
    version_manager = VersionManager()
    version_manager.logger.info("=== RAG ì„±ëŠ¥ ê°œì„  ë¹„êµ ì™„ë²½ ë¶„ì„ ì‹œì‘ v08231820 ===")
    
    # LangSmith ì„¤ì • (ê°•í™”)
    cfg = OmegaConf.create({
        'langsmith': {
            'enabled': True,
            'project_name': 'law-rag-improvement-complete-v08231820',
            'session_name': f'rag-complete-session_{datetime.now().strftime("%Y%m%d_%H%M%S")}'
        }
    })
    
    langsmith_manager = LangSmithSimple(cfg, version_manager)
    
    # ë¹„êµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    comparator = RAGImprovementComparator(version_manager, langsmith_manager)
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ì„¸íŠ¸ (í™•ì¥)
    test_questions = [
        "ì·¨ì—…ê·œì¹™ì„ ê·¼ë¡œìì—ê²Œ ë¶ˆë¦¬í•˜ê²Œ ë³€ê²½í•  ë•Œ ì‚¬ìš©ìê°€ ì§€ì¼œì•¼ í•  ë²•ì  ìš”ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "í‡´ì§ê¸ˆ ì§€ê¸‰ ê¸°ì¼ì„ ì—°ì¥í•˜ëŠ” í•©ì˜ë¥¼ í–ˆë”ë¼ë„ ì—°ì¥ëœ ê¸°ì¼ê¹Œì§€ ì§€ê¸‰í•˜ì§€ ì•Šìœ¼ë©´ í˜•ì‚¬ì²˜ë²Œì„ ë°›ë‚˜ìš”?",
        "ë¶€ë‹¹í•´ê³  êµ¬ì œì‹ ì²­ì˜ ìš”ê±´ê³¼ ì ˆì°¨ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
        "ê·¼ë¡œìì˜ ì—…ë¬´ìƒ ì¬í•´ ì¸ì • ê¸°ì¤€ê³¼ ì ˆì°¨ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
        "ì‚¬ì—…ì£¼ê°€ ê·¼ë¡œê³„ì•½ì„ í•´ì§€í•  ë•Œ ì§€ì¼œì•¼ í•  ë²•ì  ì ˆì°¨ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
    ]
    
    try:
        # ë¹„êµ ë¶„ì„ ì‹¤í–‰
        def progress_printer(progress):
            print(f"ì§„í–‰ë¥ : {progress*100:.1f}%")
        
        results = comparator.compare_models(test_questions, progress_callback=progress_printer)
        
        # ê²°ê³¼ ì €ì¥ (ë‹¤ì¤‘ í˜•ì‹)
        output_dir = ensure_directory_exists("results/rag_improvement_complete")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        json_path, csv_path, report_path = save_results_multiple_formats(results, Path(output_dir), timestamp)
        
        print(f"\nğŸ‰ RAG ì„±ëŠ¥ ê°œì„  ì™„ë²½ ë¶„ì„ ì™„ë£Œ!")
        print(f"ğŸ“„ JSON ê²°ê³¼: {json_path}")
        print(f"ğŸ“Š CSV ìš”ì•½: {csv_path}")
        print(f"ğŸ“‹ ë¶„ì„ ë³´ê³ ì„œ: {report_path}")
        
        # ìš”ì•½ ì¶œë ¥
        summary = results.get('summary', {})
        print(f"\nğŸ“ˆ ë¹ ë¥¸ ìš”ì•½:")
        for model, avg_data in summary.get('model_averages', {}).items():
            print(f"  {model}: {avg_data.get('avg_improvement_score', 0):.1f}ì  (í‰ê·  ê°œì„ )")
        
        version_manager.logger.info(f"RAG ì„±ëŠ¥ ê°œì„  ì™„ë²½ ë¶„ì„ ì™„ë£Œ - ê²°ê³¼: {json_path}")
        
        return results
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        version_manager.logger.error(f"RAG ì„±ëŠ¥ ê°œì„  ì™„ë²½ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        raise


if __name__ == "__main__":
    main()