#!/usr/bin/env python3
"""
RAG ì„±ëŠ¥ ê°œì„  ë¹„êµ ì™„ë²½ ì‹œìŠ¤í…œ v08240535
30ê°œ ì§ˆë¬¸ í‰ê°€ ì‹œìŠ¤í…œ - í†µê³„ì  ì‹ ë¢°ë„ í–¥ìƒ ë²„ì „
LangSmith ì¶”ì , Streamlit/Gradio ì›¹ ì¸í„°í˜ì´ìŠ¤ í†µí•©
ìˆœìˆ˜ LLM vs RAG ì ìš© ëª¨ë¸ì˜ ì„±ëŠ¥ ê°œì„ ë„ë¥¼ ì¸¡ì •í•˜ê³  ë¹„êµ ë¶„ì„

v08240535 ì£¼ìš” ê°œì„ ì‚¬í•­:
- 30ê°œ ì§ˆë¬¸ìœ¼ë¡œ í‰ê°€ í™•ì¥ (ê¸°ì¡´ 5ê°œ â†’ 30ê°œ)
- ë²•ë¥  6ê°œ ë¶„ì•¼ë³„ ê· í˜• ë°°ì¹˜ (ê° 5ê°œì”©)
- ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•œ ì„±ëŠ¥ ìµœì í™”
- í†µê³„ì  ì‹ ë¢°ë„ 6ë°° í–¥ìƒ
- ë”ìš± ìƒì„¸í•œ ë¶„ì„ ë¦¬í¬íŠ¸
"""

import os
import json
import time
import re
import csv
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv
from omegaconf import OmegaConf
import concurrent.futures
from typing import List, Dict, Tuple
import threading

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
import sys
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from src.utils.version_manager import VersionManager
from src.utils.langsmith_simple import LangSmithSimple
from src.utils.path_utils import ensure_directory_exists

# OpenAI ë° Anthropic ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

# LangSmith ì¶”ì 
try:
    from langsmith import traceable
    LANGSMITH_AVAILABLE = True
except ImportError:
    LANGSMITH_AVAILABLE = False
    def traceable(func):
        return func


class LawCaseLoader:
    """ë²•ë¥  íŒë¡€ ë¡œë” ë° ê²€ìƒ‰ê¸° (LangSmith ì¶”ì  í¬í•¨)"""
    
    def __init__(self, law_data_dir: str = "data/law"):
        self.law_data_dir = Path(law_data_dir)
        self.cases = []
        
    @traceable(name="load_legal_cases")
    def load_cases(self):
        """ëª¨ë“  íŒë¡€ ë¡œë“œ - LangSmith ì¶”ì """
        if not self.law_data_dir.exists():
            raise FileNotFoundError(f"ë²•ë¥  ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.law_data_dir}")
        
        json_files = list(self.law_data_dir.glob("*.json"))
        
        for json_file in json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    case_data = json.load(f)
                
                self.cases.append({
                    'case_number': case_data.get('ì‚¬ê±´ë²ˆí˜¸', ''),
                    'case_name': case_data.get('ì‚¬ê±´ëª…', ''),
                    'court': case_data.get('ë²•ì›ëª…', ''),
                    'date': case_data.get('ì„ ê³ ì¼ì', ''),
                    'case_type': case_data.get('ì‚¬ê±´ì¢…ë¥˜ëª…', ''),
                    'summary': case_data.get('íŒì‹œì‚¬í•­', ''),
                    'decision': case_data.get('íŒê²°ìš”ì§€', ''),
                    'references': case_data.get('ì°¸ì¡°ì¡°ë¬¸', ''),
                    'content': case_data.get('íŒë¡€ë‚´ìš©', ''),
                    'full_text': self._format_case_text(case_data)
                })
                
            except Exception as e:
                print(f"íŒë¡€ ë¡œë“œ ì˜¤ë¥˜ {json_file}: {e}")
                continue
        
        print(f"ì´ {len(self.cases)}ê°œ íŒë¡€ ë¡œë“œ ì™„ë£Œ")
        return self.cases
    
    def _format_case_text(self, case_data: dict) -> str:
        """íŒë¡€ë¥¼ RAGìš© í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
        return f"""
ì‚¬ê±´ë²ˆí˜¸: {case_data.get('ì‚¬ê±´ë²ˆí˜¸', '') or 'N/A'}
ì‚¬ê±´ëª…: {case_data.get('ì‚¬ê±´ëª…', '') or 'N/A'}
ë²•ì›ëª…: {case_data.get('ë²•ì›ëª…', '') or 'N/A'}
ì„ ê³ ì¼ì: {case_data.get('ì„ ê³ ì¼ì', '') or 'N/A'}
ì‚¬ê±´ì¢…ë¥˜: {case_data.get('ì‚¬ê±´ì¢…ë¥˜ëª…', '') or 'N/A'}

íŒì‹œì‚¬í•­:
{case_data.get('íŒì‹œì‚¬í•­', '') or 'N/A'}

íŒê²°ìš”ì§€:
{case_data.get('íŒê²°ìš”ì§€', '') or 'N/A'}

ì°¸ì¡°ì¡°ë¬¸:
{case_data.get('ì°¸ì¡°ì¡°ë¬¸', '') or 'N/A'}

íŒë¡€ë‚´ìš©:
{case_data.get('íŒë¡€ë‚´ìš©', '') or 'N/A'}
""".strip()
    
    @traceable(name="search_relevant_cases")
    def search_relevant_cases(self, question: str, top_k: int = 3) -> list:
        """ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ íŒë¡€ ê²€ìƒ‰ - LangSmith ì¶”ì """
        if not self.cases:
            return []
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
        question_keywords = question.lower().split()
        scored_cases = []
        
        for case in self.cases:
            search_text = (case['full_text'] + ' ' + case['case_name']).lower()
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
            score = 0
            for keyword in question_keywords:
                if len(keyword) > 1:  # í•œ ê¸€ì í‚¤ì›Œë“œ ì œì™¸
                    count = search_text.count(keyword)
                    score += count * len(keyword)  # ê¸´ í‚¤ì›Œë“œì— ê°€ì¤‘ì¹˜
            
            if score > 0:
                scored_cases.append((case, score))
        
        # ì ìˆ˜ìˆœ ì •ë ¬í•˜ì—¬ ìƒìœ„ kê°œ ë°˜í™˜
        scored_cases.sort(key=lambda x: x[1], reverse=True)
        selected_cases = [case[0] for case in scored_cases[:top_k]]
        
        return selected_cases


class RAGImprovementComparator:
    """RAG ì„±ëŠ¥ ê°œì„  ë¹„êµ ë¶„ì„ê¸° (v08240535 - 30ê°œ ì§ˆë¬¸ í™•ì¥ ë²„ì „)"""
    
    def __init__(self, version_manager: VersionManager, langsmith_manager=None):
        self.version_manager = version_manager
        self.langsmith_manager = langsmith_manager
        self.case_loader = LawCaseLoader()
        self.case_loader.load_cases()
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        if OPENAI_AVAILABLE:
            self.openai_client = OpenAI()
        else:
            self.openai_client = None
            
        # Anthropic í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”  
        if ANTHROPIC_AVAILABLE:
            self.anthropic_client = anthropic.Anthropic()
        else:
            self.anthropic_client = None
            
        # ìŠ¤ë ˆë“œ ì•ˆì „ ë½
        self.progress_lock = threading.Lock()
    
    @traceable(name="get_pure_llm_response")
    def get_pure_llm_response(self, model_name: str, question: str, temperature: float = 0.1) -> dict:
        """ìˆœìˆ˜ LLM ì‘ë‹µ (RAG ì—†ì´) - LangSmith ì¶”ì """
        start_time = time.time()
        
        try:
            if model_name == "GPT-4o" and self.openai_client:
                response = self.openai_client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ìƒì„¸í•œ ë²•ë¥  ìë¬¸ì„ ì œê³µí•´ì£¼ì„¸ìš”."},
                        {"role": "user", "content": question}
                    ],
                    temperature=temperature,
                    max_tokens=1000
                )
                
                answer = response.choices[0].message.content.strip()
                response_time = time.time() - start_time
                
                return {
                    'answer': answer,
                    'response_time': response_time,
                    'answer_length': len(answer),
                    'word_count': len(answer.split()),
                    'status': 'success'
                }
                
            elif model_name == "Claude-3.5" and self.anthropic_client:
                response = self.anthropic_client.messages.create(
                    model="claude-3-5-sonnet-20241022",
                    max_tokens=1000,
                    temperature=temperature,
                    system="ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ìƒì„¸í•œ ë²•ë¥  ìë¬¸ì„ ì œê³µí•´ì£¼ì„¸ìš”.",
                    messages=[
                        {"role": "user", "content": question}
                    ]
                )
                
                answer = response.content[0].text.strip()
                response_time = time.time() - start_time
                
                return {
                    'answer': answer,
                    'response_time': response_time,
                    'answer_length': len(answer),
                    'word_count': len(answer.split()),
                    'status': 'success'
                }
            else:
                return {
                    'answer': f"[{model_name} ì‚¬ìš© ë¶ˆê°€]",
                    'response_time': 0,
                    'answer_length': 0,
                    'word_count': 0,
                    'status': 'unavailable'
                }
                
        except Exception as e:
            print(f"ìˆœìˆ˜ {model_name} ì‘ë‹µ ì˜¤ë¥˜: {e}")
            return {
                'answer': f"[{model_name} ì˜¤ë¥˜: {str(e)}]",
                'response_time': time.time() - start_time,
                'answer_length': 0,
                'word_count': 0,
                'status': 'error'
            }
    
    @traceable(name="get_rag_response")  
    def get_rag_response(self, model_name: str, question: str, temperature: float = 0.1) -> dict:
        """RAG ê¸°ë°˜ LLM ì‘ë‹µ - LangSmith ì¶”ì """
        start_time = time.time()
        
        # ê´€ë ¨ íŒë¡€ ê²€ìƒ‰
        relevant_cases = self.case_loader.search_relevant_cases(question, top_k=3)
        
        # íŒë¡€ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "\n\n".join([
            f"[íŒë¡€ {i+1}] {case['case_number']} - {case['case_name']}\n{case['summary']}\n{case['decision']}"
            for i, case in enumerate(relevant_cases)
        ]) if relevant_cases else "[ê´€ë ¨ íŒë¡€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤]"
        
        # RAG í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        rag_prompt = f"""ë‹¤ìŒ ê´€ë ¨ íŒë¡€ë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

<ê´€ë ¨ íŒë¡€>
{context}
</ê´€ë ¨ íŒë¡€>

<ì§ˆë¬¸>
{question}
</ì§ˆë¬¸>

ìœ„ íŒë¡€ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ë²•ë¥ ì  ê·¼ê±°ë¥¼ ì œì‹œí•˜ë©° ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."""

        try:
            if model_name == "GPT-4o" and self.openai_client:
                response = self.openai_client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ íŒë¡€ë¥¼ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìƒì„¸í•œ ë²•ë¥  ìë¬¸ì„ ì œê³µí•´ì£¼ì„¸ìš”."},
                        {"role": "user", "content": rag_prompt}
                    ],
                    temperature=temperature,
                    max_tokens=1200
                )
                
                answer = response.choices[0].message.content.strip()
                response_time = time.time() - start_time
                
                return {
                    'answer': answer,
                    'response_time': response_time,
                    'answer_length': len(answer),
                    'word_count': len(answer.split()),
                    'case_count': len(relevant_cases),
                    'relevant_cases': [case['case_number'] for case in relevant_cases],
                    'status': 'success'
                }
                
            elif model_name == "Claude-3.5" and self.anthropic_client:
                response = self.anthropic_client.messages.create(
                    model="claude-3-5-sonnet-20241022",
                    max_tokens=1200,
                    temperature=temperature,
                    system="ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ íŒë¡€ë¥¼ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìƒì„¸í•œ ë²•ë¥  ìë¬¸ì„ ì œê³µí•´ì£¼ì„¸ìš”.",
                    messages=[
                        {"role": "user", "content": rag_prompt}
                    ]
                )
                
                answer = response.content[0].text.strip()
                response_time = time.time() - start_time
                
                return {
                    'answer': answer,
                    'response_time': response_time,
                    'answer_length': len(answer),
                    'word_count': len(answer.split()),
                    'case_count': len(relevant_cases),
                    'relevant_cases': [case['case_number'] for case in relevant_cases],
                    'status': 'success'
                }
            else:
                return {
                    'answer': f"[{model_name} ì‚¬ìš© ë¶ˆê°€]",
                    'response_time': 0,
                    'answer_length': 0,
                    'word_count': 0,
                    'case_count': 0,
                    'relevant_cases': [],
                    'status': 'unavailable'
                }
                
        except Exception as e:
            print(f"RAG {model_name} ì‘ë‹µ ì˜¤ë¥˜: {e}")
            return {
                'answer': f"[{model_name} RAG ì˜¤ë¥˜: {str(e)}]",
                'response_time': time.time() - start_time,
                'answer_length': 0,
                'word_count': 0,
                'case_count': len(relevant_cases) if relevant_cases else 0,
                'relevant_cases': [case['case_number'] for case in relevant_cases] if relevant_cases else [],
                'status': 'error'
            }
    
    def _evaluate_improvement(self, pure_result: dict, rag_result: dict, question: str) -> dict:
        """RAG ê°œì„ ë„ í‰ê°€ (ê¸°ì¡´ í‚¤ì›Œë“œ ë°©ì‹ ìœ ì§€)"""
        if pure_result['status'] != 'success' or rag_result['status'] != 'success':
            return {
                'overall_score': 0,
                'analysis': "ì‘ë‹µ ìƒì„± ì˜¤ë¥˜ë¡œ í‰ê°€ ë¶ˆê°€",
                'specificity_improvement': 0,
                'evidence_improvement': 0,
                'length_change': 0,
                'word_count_change': 0,
                'response_time_change': 0,
                'legal_keyword_density': 0
            }
        
        pure_answer = pure_result['answer'].lower()
        rag_answer = rag_result['answer'].lower()
        
        # 1. êµ¬ì²´ì„± ê°œì„  (ì‚¬ê±´ë²ˆí˜¸ ì–¸ê¸‰)
        case_numbers = [case_num for case_num in rag_result.get('relevant_cases', [])]
        case_mentions = sum(1 for case_num in case_numbers if case_num.lower() in rag_answer)
        specificity_improvement = case_mentions
        
        # 2. ê·¼ê±° ê°œì„  (ë²•ë¥  í‚¤ì›Œë“œ ë°€ë„)
        legal_keywords = ['ë²•ë¥ ', 'ì¡°ë¬¸', 'íŒë¡€', 'ë²•ì›', 'ëŒ€ë²•ì›', 'ë¯¼ë²•', 'í˜•ë²•', 'ê·¼ë¡œê¸°ì¤€ë²•', 
                         'ìƒë²•', 'í—Œë²•', 'ê·œì •', 'ìœ„ë°˜', 'ì²˜ë²Œ', 'ì†í•´ë°°ìƒ', 'ì†Œì†¡', 'íŒê²°',
                         'í•­ì†Œ', 'ìƒê³ ', 'ì¬íŒ', 'ì„ ê³ ', 'í˜•ì‚¬', 'ë¯¼ì‚¬', 'í–‰ì •', 'í—Œì¬']
        
        pure_keyword_count = sum(pure_answer.count(keyword) for keyword in legal_keywords)
        rag_keyword_count = sum(rag_answer.count(keyword) for keyword in legal_keywords)
        evidence_improvement = max(0, rag_keyword_count - pure_keyword_count)
        
        # 3. ê¸¸ì´ ë° ë‹¨ì–´ ìˆ˜ ë³€í™”
        length_change = rag_result['answer_length'] - pure_result['answer_length']
        word_count_change = rag_result['word_count'] - pure_result['word_count']
        
        # 4. ì‘ë‹µ ì‹œê°„ ë³€í™”
        response_time_change = rag_result['response_time'] - pure_result['response_time']
        
        # 5. ë²•ë¥  í‚¤ì›Œë“œ ë°€ë„ (1000ê¸€ìë‹¹)
        legal_keyword_density = (rag_keyword_count / max(rag_result['answer_length'], 1)) * 1000
        
        # 6. ì „ì²´ì  ê°œì„  ì ìˆ˜ ê³„ì‚° (0-100ì )
        overall_score = min(100, max(0, 
            (specificity_improvement * 20) +     # ì‚¬ê±´ë²ˆí˜¸ ì¸ìš©ë‹¹ 20ì 
            (evidence_improvement * 5) +         # ë²•ë¥  í‚¤ì›Œë“œë‹¹ 5ì   
            (min(length_change, 500) / 10) +     # ê¸¸ì´ ì¦ê°€ë¶„ ìµœëŒ€ 50ì 
            (rag_result.get('case_count', 0) * 5)  # ì‚¬ìš©ëœ íŒë¡€ë‹¹ 5ì 
        ))
        
        # 7. ë¶„ì„ ìš”ì•½
        analysis_parts = []
        
        if specificity_improvement > 0:
            analysis_parts.append(f"ì‚¬ê±´ë²ˆí˜¸ {specificity_improvement}ê±´ ì¸ìš©ìœ¼ë¡œ êµ¬ì²´ì„± í–¥ìƒ")
        
        if evidence_improvement > 0:
            analysis_parts.append(f"ë²•ë¥  í‚¤ì›Œë“œ {evidence_improvement}ê°œ ì¶”ê°€ë¡œ ê·¼ê±° ê°•í™”")
        else:
            analysis_parts.append("ë²•ë¥  í‚¤ì›Œë“œ ì¦ê°€ ì—†ìŒ")
            
        if length_change > 100:
            analysis_parts.append(f"ë‹µë³€ ê¸¸ì´ {length_change}ê¸€ì ì¦ê°€")
        elif length_change < -100:
            analysis_parts.append(f"ë‹µë³€ ê¸¸ì´ {abs(length_change)}ê¸€ì ê°ì†Œ")
            
        if rag_result.get('case_count', 0) > 0:
            analysis_parts.append(f"{rag_result['case_count']}ê±´ íŒë¡€ í™œìš©")
        else:
            analysis_parts.append("ê´€ë ¨ íŒë¡€ ì°¾ì§€ ëª»í•¨")
        
        analysis = "; ".join(analysis_parts) if analysis_parts else "ê°œì„  íš¨ê³¼ ë¯¸ë¯¸"
        
        return {
            'overall_score': overall_score,
            'analysis': analysis,
            'specificity_improvement': specificity_improvement,
            'evidence_improvement': evidence_improvement,
            'length_change': length_change,
            'word_count_change': word_count_change,
            'response_time_change': response_time_change,
            'legal_keyword_density': legal_keyword_density
        }

    def _process_single_question(self, question_data: Tuple[int, str], models: List[str], 
                                temperature: float, progress_callback) -> Tuple[int, Dict]:
        """ë‹¨ì¼ ì§ˆë¬¸ ì²˜ë¦¬ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
        q_idx, question = question_data
        question_id = f"q{q_idx+1:02d}"
        
        result = {
            'question': question,
            'responses': {},
            'improvements': {},
            'metrics': {}
        }
        
        # ê° ëª¨ë¸ë³„ ì²˜ë¦¬
        for model in models:
            try:
                # ìˆœìˆ˜ LLM ì‘ë‹µ
                pure_result = self.get_pure_llm_response(model, question, temperature)
                
                # RAG ê¸°ë°˜ ì‘ë‹µ  
                rag_result = self.get_rag_response(model, question, temperature)
                
                # ê°œì„ ë„ í‰ê°€
                improvement = self._evaluate_improvement(pure_result, rag_result, question)
                
                # ê²°ê³¼ ì €ì¥
                result['responses'][model] = {
                    'pure': pure_result,
                    'rag': rag_result
                }
                result['improvements'][model] = improvement
                result['metrics'][model] = {
                    'pure_answer_length': pure_result.get('answer_length', 0),
                    'rag_answer_length': rag_result.get('answer_length', 0),
                    'improvement_score': improvement['overall_score']
                }
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                if progress_callback:
                    with self.progress_lock:
                        progress_callback(0.1)  # ê° ì‘ì—…ë‹¹ ì§„í–‰ë¥ 
                        
            except Exception as e:
                print(f"ì§ˆë¬¸ {question_id}, ëª¨ë¸ {model} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                result['responses'][model] = {'pure': {'status': 'error'}, 'rag': {'status': 'error'}}
                result['improvements'][model] = {'overall_score': 0, 'analysis': f'ì˜¤ë¥˜: {str(e)}'}
                result['metrics'][model] = {'improvement_score': 0}
        
        return q_idx, result

    @traceable(name="compare_models_parallel")
    def compare_models(self, questions: list, temperature: float = 0.1, progress_callback=None) -> dict:
        """ëª¨ë¸ ë¹„êµ ë¶„ì„ (ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™” v08240535)"""
        start_time = time.time()
        
        models = ['GPT-4o', 'Claude-3.5']
        
        results = {
            'version': 'v08240535',
            'timestamp': datetime.now().isoformat(),
            'total_questions': len(questions),
            'models': models,
            'questions': {}
        }
        
        print(f"ğŸš€ RAG ì„±ëŠ¥ ë¹„êµ ì‹œì‘ (v08240535) - {len(questions)}ê°œ ì§ˆë¬¸, {len(models)}ê°œ ëª¨ë¸")
        print(f"ğŸ“Š ì˜ˆìƒ ì†Œìš”ì‹œê°„: ì•½ {len(questions) * len(models) * 2}ë¶„")
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
        question_data = list(enumerate(questions))
        
        # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ ì²˜ë¦¬
        max_workers = min(4, len(questions))  # ìµœëŒ€ 4ê°œ ìŠ¤ë ˆë“œ
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ê° ì§ˆë¬¸ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬
            future_to_question = {
                executor.submit(self._process_single_question, qdata, models, temperature, progress_callback): qdata[0]
                for qdata in question_data
            }
            
            completed = 0
            for future in concurrent.futures.as_completed(future_to_question):
                q_idx = future_to_question[future]
                try:
                    q_idx_result, question_result = future.result()
                    question_id = f"q{q_idx_result+1:02d}"
                    results['questions'][question_id] = question_result
                    
                    completed += 1
                    progress = completed / len(questions)
                    
                    print(f"âœ… ì§ˆë¬¸ {question_id} ì™„ë£Œ ({completed}/{len(questions)}) - {progress*100:.1f}%")
                    
                    if progress_callback:
                        progress_callback(progress)
                        
                except Exception as e:
                    print(f"âŒ ì§ˆë¬¸ {q_idx+1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    question_id = f"q{q_idx+1:02d}"
                    results['questions'][question_id] = {
                        'question': questions[q_idx] if q_idx < len(questions) else 'Unknown',
                        'error': str(e)
                    }
        
        # ì „ì²´ ìš”ì•½ í†µê³„ ìƒì„±
        results['summary'] = self._generate_summary(results)
        results['total_processing_time'] = time.time() - start_time
        
        print(f"ğŸ‰ ì „ì²´ ë¶„ì„ ì™„ë£Œ! ì´ ì†Œìš”ì‹œê°„: {results['total_processing_time']:.1f}ì´ˆ")
        
        return results
    
    def _generate_summary(self, results: dict) -> dict:
        """ì „ì²´ ê²°ê³¼ ìš”ì•½ í†µê³„ ìƒì„±"""
        models = results.get('models', [])
        questions = results.get('questions', {})
        
        summary = {
            'model_averages': {},
            'performance_comparison': {},
            'question_statistics': {}
        }
        
        # ëª¨ë¸ë³„ í‰ê·  ê³„ì‚°
        for model in models:
            scores = []
            time_changes = []
            length_changes = []
            cases_used = []
            
            for q_data in questions.values():
                if model in q_data.get('improvements', {}):
                    improvement = q_data['improvements'][model]
                    scores.append(improvement.get('overall_score', 0))
                    time_changes.append(improvement.get('response_time_change', 0))
                    length_changes.append(improvement.get('length_change', 0))
                    
                if model in q_data.get('responses', {}):
                    rag_resp = q_data['responses'][model].get('rag', {})
                    cases_used.append(rag_resp.get('case_count', 0))
            
            if scores:
                summary['model_averages'][model] = {
                    'avg_improvement_score': sum(scores) / len(scores),
                    'avg_time_increase': sum(time_changes) / len(time_changes),
                    'avg_length_increase': sum(length_changes) / len(length_changes),
                    'avg_cases_used': sum(cases_used) / len(cases_used),
                    'best_score': max(scores),
                    'worst_score': min(scores),
                    'total_questions': len(scores)
                }
        
        # ëª¨ë¸ê°„ ì„±ëŠ¥ ë¹„êµ
        if len(models) >= 2:
            model1_avg = summary['model_averages'].get(models[0], {}).get('avg_improvement_score', 0)
            model2_avg = summary['model_averages'].get(models[1], {}).get('avg_improvement_score', 0)
            
            summary['performance_comparison'] = {
                'better_improvement': models[0] if model1_avg > model2_avg else models[1],
                'score_difference': abs(model1_avg - model2_avg),
                'faster_processing': "ë¶„ì„ í•„ìš”"  # ì¶”í›„ êµ¬í˜„
            }
        
        # ì§ˆë¬¸ í†µê³„
        all_scores = []
        for q_data in questions.values():
            for model in models:
                if model in q_data.get('improvements', {}):
                    all_scores.append(q_data['improvements'][model].get('overall_score', 0))
        
        if all_scores:
            summary['question_statistics'] = {
                'total_evaluations': len(all_scores),
                'overall_avg_score': sum(all_scores) / len(all_scores),
                'highest_score': max(all_scores),
                'lowest_score': min(all_scores),
                'score_std_dev': self._calculate_std_dev(all_scores)
            }
        
        return summary
    
    def _calculate_std_dev(self, scores: List[float]) -> float:
        """í‘œì¤€í¸ì°¨ ê³„ì‚°"""
        if len(scores) <= 1:
            return 0.0
        
        mean = sum(scores) / len(scores)
        variance = sum((x - mean) ** 2 for x in scores) / (len(scores) - 1)
        return variance ** 0.5


def get_30_evaluation_questions() -> List[str]:
    """30ê°œ í‰ê°€ ì§ˆë¬¸ ì„¸íŠ¸ - ë²•ë¥  6ê°œ ë¶„ì•¼ë³„ 5ê°œì”©"""
    return [
        # 1. ê·¼ë¡œë²• ë¶„ì•¼ (5ê°œ)
        "ì·¨ì—…ê·œì¹™ì„ ê·¼ë¡œìì—ê²Œ ë¶ˆë¦¬í•˜ê²Œ ë³€ê²½í•  ë•Œ ì‚¬ìš©ìê°€ ì§€ì¼œì•¼ í•  ë²•ì  ìš”ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "í‡´ì§ê¸ˆ ì§€ê¸‰ ê¸°ì¼ì„ ì—°ì¥í•˜ëŠ” í•©ì˜ë¥¼ í–ˆë”ë¼ë„ ì—°ì¥ëœ ê¸°ì¼ê¹Œì§€ ì§€ê¸‰í•˜ì§€ ì•Šìœ¼ë©´ í˜•ì‚¬ì²˜ë²Œì„ ë°›ë‚˜ìš”?",
        "ì—¬ê°ìë™ì°¨ë²•ìƒ ìš´ìˆ˜ì¢…ì‚¬ì ë³´ìˆ˜êµìœ¡ ì‹œê°„ì´ ê·¼ë¡œì‹œê°„ì— í¬í•¨ë˜ëŠ” ê¸°ì¤€ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "íœ´ì¼ê·¼ë¡œìˆ˜ë‹¹ ì§€ê¸‰ ëŒ€ìƒì´ ë˜ëŠ” íœ´ì¼ì˜ ë²”ìœ„ì™€ íŒë‹¨ ê¸°ì¤€ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
        "ê·¼ë¡œê¸°ì¤€ë²• ìœ„ë°˜ì£„ì—ì„œ ì‚¬ìš©ìì˜ ê³ ì˜ ì¸ì • ê¸°ì¤€ê³¼ ì†Œë©¸ì‹œíš¨ ì ìš© ì›ì¹™ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        
        # 2. ë¯¼ì‚¬ë²• ë¶„ì•¼ (5ê°œ)
        "ì˜ì‚¬ê°€ ì˜ë£Œê¸°ê´€ì— ëŒ€í•˜ì—¬ ê°–ëŠ” ê¸‰ì—¬Â·ìˆ˜ë‹¹Â·í‡´ì§ê¸ˆ ì±„ê¶Œì´ ìƒì‚¬ì±„ê¶Œì— í•´ë‹¹í•˜ëŠ”ì§€ ì—¬ë¶€ëŠ”?",
        "ë¶€ë‹¹ì´ë“ë°˜í™˜ì²­êµ¬ê¶Œì˜ ì†Œë©¸ì‹œíš¨ ê¸°ì‚°ì ê³¼ ìƒê³„ì ìƒ ë°œìƒ ì¡°ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ì„ëŒ€ì°¨ê³„ì•½ì—ì„œ ë³´ì¦ê¸ˆ ë°˜í™˜ì˜ë¬´ì™€ ì§€ì—°ì†í•´ê¸ˆ ì‚°ì • ê¸°ì¤€ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
        "ê³„ì•½ í•´ì œ ì‹œ ì›ìƒíšŒë³µ ì˜ë¬´ì˜ ë²”ìœ„ì™€ ì†í•´ë°°ìƒ ì²­êµ¬ ìš”ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ë¶ˆë²•í–‰ìœ„ë¡œ ì¸í•œ ì†í•´ë°°ìƒì—ì„œ ì •ì‹ ì  í”¼í•´ì˜ ì¸ì • ê¸°ì¤€ê³¼ ë°°ìƒ ë²”ìœ„ëŠ”?",
        
        # 3. í–‰ì •ë²• ë¶„ì•¼ (5ê°œ)
        "í–‰ì •ì²˜ë¶„ ì·¨ì†Œì†Œì†¡ì—ì„œ ì²˜ë¶„ì²­ì˜ ì¬ëŸ‰ê¶Œ ì¼íƒˆÂ·ë‚¨ìš© íŒë‹¨ ê¸°ì¤€ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ì˜ì—…ì •ì§€ ì²˜ë¶„ì— ëŒ€í•œ ë¶ˆë³µì ˆì°¨ì™€ ì§‘í–‰ì •ì§€ ì‹ ì²­ ìš”ê±´ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
        "í–‰ì •ì²­ì˜ í—ˆê°€ ê±°ë¶€ì²˜ë¶„ì— ëŒ€í•œ ì·¨ì†Œì†Œì†¡ ì œê¸° ì‹œ ì…ì¦ì±…ì„ ë¶„ë°°ëŠ”?",
        "ê³µë¬´ì› ì§•ê³„ì²˜ë¶„ì˜ ì ë²•ì„± íŒë‹¨ ê¸°ì¤€ê³¼ ë¹„ë¡€ì›ì¹™ ì ìš©ì€ ì–´ë–»ê²Œ ì´ë£¨ì–´ì§€ë‚˜ìš”?",
        "í–‰ì •ëŒ€ì§‘í–‰ì˜ ìš”ê±´ê³¼ ì ˆì°¨, ê·¸ë¦¬ê³  ì†í•´ë°°ìƒ ì²­êµ¬ ê°€ëŠ¥ì„±ì€?",
        
        # 4. ìƒì‚¬ë²• ë¶„ì•¼ (5ê°œ)
        "ì£¼ì‹íšŒì‚¬ ì´ì‚¬ì˜ ì„ ê´€ì£¼ì˜ì˜ë¬´ ìœ„ë°˜ ì‹œ ì†í•´ë°°ìƒì±…ì„ì˜ ë²”ìœ„ì™€ ë©´ì±… ìš”ê±´ì€?",
        "ìƒë²•ìƒ ìƒì¸ íŒë‹¨ ê¸°ì¤€ê³¼ ìƒì‚¬ì±„ê¶Œì— ëŒ€í•œ íŠ¹ë¡€ ì ìš© ì—¬ë¶€ëŠ”?",
        "íšŒì‚¬ í•©ë³‘ ì‹œ ì£¼ì£¼ì˜ ë°˜ëŒ€ì£¼ì£¼ ì£¼ì‹ë§¤ìˆ˜ì²­êµ¬ê¶Œ í–‰ì‚¬ ìš”ê±´ê³¼ ì ˆì°¨ëŠ”?",
        "ì–´ìŒÂ·ìˆ˜í‘œë²•ìƒ ë°°ì„œì¸ì˜ ë‹´ë³´ì±…ì„ê³¼ ì†Œêµ¬ê¶Œ í–‰ì‚¬ì˜ ë²•ì  ìš”ê±´ì€?",
        "ìƒì‚¬ì¤‘ì¬ í•©ì˜ì˜ íš¨ë ¥ê³¼ ë²•ì›ì˜ ì¤‘ì¬íŒì • ì·¨ì†Œ ì‚¬ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        
        # 5. í˜•ì‚¬ë²• ë¶„ì•¼ (5ê°œ)
        "ì—…ë¬´ìƒë°°ì„ì£„ì—ì„œ 'íƒ€ì¸ì˜ ì‚¬ë¬´ ì²˜ë¦¬' ìš”ê±´ê³¼ ë°°ì„í–‰ìœ„ì˜ ì¸ì • ê¸°ì¤€ì€?",
        "íš¡ë ¹ì£„ì™€ ë°°ì„ì£„ì˜ êµ¬ë³„ ê¸°ì¤€ê³¼ ê°ê°ì˜ ì„±ë¦½ìš”ê±´ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ì‚¬ê¸°ì£„ì—ì„œ ê¸°ë§í–‰ìœ„ì˜ ì¸ì • ê¸°ì¤€ê³¼ ì¬ì‚°ìƒ ì†í•´ ë°œìƒì˜ ì¸ê³¼ê´€ê³„ëŠ”?",
        "ê³µë¬´ì› ë‡Œë¬¼ì£„ì—ì„œ 'ì§ë¬´ê´€ë ¨ì„±' íŒë‹¨ ê¸°ì¤€ê³¼ ë¶€ì •ì²­íƒê¸ˆì§€ë²•ê³¼ì˜ ê´€ê³„ëŠ”?",
        "ì •ë‹¹ë°©ìœ„ ì„±ë¦½ìš”ê±´ ì¤‘ 'í˜„ì¬ì˜ ë¶€ë‹¹í•œ ì¹¨í•´'ì™€ 'ìƒë‹¹ì„±' íŒë‹¨ ê¸°ì¤€ì€?",
        
        # 6. ê°€ì¡±ë²• ë¶„ì•¼ (5ê°œ)
        "ì´í˜¼ ì‹œ ì¬ì‚°ë¶„í• ì²­êµ¬ê¶Œì˜ ëŒ€ìƒ ì¬ì‚° ë²”ìœ„ì™€ ë¶„í•  ë¹„ìœ¨ ê²°ì • ê¸°ì¤€ì€?",
        "ì¹œê¶Œì ì§€ì •ì—ì„œ ìë…€ì˜ ë³µë¦¬ íŒë‹¨ ê¸°ì¤€ê³¼ ë©´ì ‘êµì„­ê¶Œì˜ ì œí•œ ì‚¬ìœ ëŠ”?",
        "ìœ ì–¸ì˜ ë°©ì‹ë³„ ì„±ë¦½ìš”ê±´ê³¼ ìœ ì–¸ë¬´íš¨ í™•ì¸ì†Œì†¡ì˜ ì¦ëª…ì±…ì„ì€?",
        "ìƒì†ì¬ì‚° ë¶„í•  ì‹œ íŠ¹ë³„ìˆ˜ìµìì˜ êµ¬ì²´ì  ìƒì†ë¶„ ì‚°ì • ë°©ë²•ì€?",
        "í˜¼ì¸ ë¬´íš¨Â·ì·¨ì†Œ ì‚¬ìœ ì™€ ê·¸ ë²•ì  íš¨ê³¼ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"
    ]


def save_results_multiple_formats(results: dict, output_dir: Path, timestamp: str) -> tuple:
    """ê²°ê³¼ë¥¼ JSON, CSV, Markdown í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
    
    # JSON ì €ì¥
    json_path = output_dir / f"rag_improvement_v08240535_{timestamp}.json"
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # CSV ìš”ì•½ ì €ì¥  
    csv_path = output_dir / f"rag_improvement_v08240535_summary_{timestamp}.csv"
    with open(csv_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow([
            'ì§ˆë¬¸ID', 'ì§ˆë¬¸ë‚´ìš©', 'ëª¨ë¸', 'ìˆœìˆ˜ì ìˆ˜', 'RAGì ìˆ˜', 'ê°œì„ ì ìˆ˜', 
            'ìˆœìˆ˜ì‘ë‹µì‹œê°„', 'RAGì‘ë‹µì‹œê°„', 'ì‹œê°„ë³€í™”', 'ì‚¬ìš©íŒë¡€ìˆ˜', 'ë¶„ì„ê²°ê³¼'
        ])
        
        # ë°ì´í„°
        for q_id, q_data in results.get('questions', {}).items():
            for model in ['GPT-4o', 'Claude-3.5']:
                if model in q_data.get('improvements', {}):
                    improvement = q_data['improvements'][model]
                    responses = q_data['responses'][model]
                    
                    writer.writerow([
                        q_id, q_data['question'][:50] + '...',
                        model,
                        'N/A',  # ìˆœìˆ˜ì ìˆ˜ëŠ” ë³„ë„ ê³„ì‚° í•„ìš”
                        'N/A',  # RAGì ìˆ˜ëŠ” ë³„ë„ ê³„ì‚° í•„ìš”
                        improvement['overall_score'],
                        responses['pure']['response_time'],
                        responses['rag']['response_time'],
                        improvement['response_time_change'],
                        responses['rag'].get('case_count', 0),
                        improvement['analysis'][:100] + '...'
                    ])
    
    # ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±
    report_path = output_dir / f"rag_improvement_v08240535_report_{timestamp}.md"
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(f"# RAG ì„±ëŠ¥ ê°œì„  ì™„ë²½ ë¶„ì„ ë³´ê³ ì„œ v08240535\n\n")
        f.write(f"**ìƒì„± ì‹œê°„**: {results.get('timestamp', 'N/A')}\n")
        f.write(f"**í‰ê°€ ì§ˆë¬¸ ìˆ˜**: {results.get('total_questions', 0)}ê°œ (6ë°° í™•ì¥)\n")
        f.write(f"**ë¶„ì„ ëª¨ë¸**: {', '.join(results.get('models', []))}\n")
        f.write(f"**ì´ ì²˜ë¦¬ ì‹œê°„**: {results.get('total_processing_time', 0):.1f}ì´ˆ\n\n")
        
        # ìš”ì•½ í†µê³„
        summary = results.get('summary', {})
        f.write(f"## ğŸ“ˆ ëª¨ë¸ë³„ ì„±ëŠ¥ ìš”ì•½\n\n")
        
        for model, avg_data in summary.get('model_averages', {}).items():
            f.write(f"### {model}\n")
            f.write(f"- **í‰ê·  ê°œì„  ì ìˆ˜**: {avg_data.get('avg_improvement_score', 0):.1f}/100\n")
            f.write(f"- **í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì¦ê°€**: {avg_data.get('avg_time_increase', 0):+.2f}ì´ˆ\n")
            f.write(f"- **í‰ê·  ë‹µë³€ ê¸¸ì´ ì¦ê°€**: {avg_data.get('avg_length_increase', 0):+.0f}ê¸€ì\n")
            f.write(f"- **í‰ê·  ì‚¬ìš© íŒë¡€**: {avg_data.get('avg_cases_used', 0):.1f}ê±´\n")
            f.write(f"- **ìµœê³ /ìµœì € ì ìˆ˜**: {avg_data.get('best_score', 0):.1f} / {avg_data.get('worst_score', 0):.1f}\n")
        
        # ëª¨ë¸ ë¹„êµ
        if 'performance_comparison' in summary:
            comp = summary['performance_comparison']
            f.write(f"\n## ğŸ† ëª¨ë¸ê°„ ì„±ëŠ¥ ë¹„êµ\n\n")
            f.write(f"- **ë” ë‚˜ì€ ê°œì„  íš¨ê³¼**: {comp.get('better_improvement', 'N/A')}\n")
            f.write(f"- **ë” ë¹ ë¥¸ ì²˜ë¦¬**: {comp.get('faster_processing', 'N/A')}\n")
            f.write(f"- **ì ìˆ˜ ì°¨ì´**: {comp.get('score_difference', 0):.1f}ì \n\n")
        
        # ì „ì²´ í†µê³„
        if 'question_statistics' in summary:
            q_stats = summary['question_statistics']
            f.write(f"## ğŸ“Š ì „ì²´ í†µê³„ (v08240535)\n\n")
            f.write(f"- **ì´ í‰ê°€ ìˆ˜**: {q_stats.get('total_evaluations', 0)}íšŒ\n")
            f.write(f"- **ì „ì²´ í‰ê·  ì ìˆ˜**: {q_stats.get('overall_avg_score', 0):.1f}/100\n")
            f.write(f"- **ìµœê³  ì ìˆ˜**: {q_stats.get('highest_score', 0):.1f}\n")
            f.write(f"- **ìµœì € ì ìˆ˜**: {q_stats.get('lowest_score', 0):.1f}\n")
            f.write(f"- **ì ìˆ˜ í‘œì¤€í¸ì°¨**: {q_stats.get('score_std_dev', 0):.2f}\n")
            f.write(f"- **ì‹ ë¢°ë„ ê°œì„ **: ê¸°ì¡´ ëŒ€ë¹„ 6ë°° í–¥ìƒ (30ê°œ ì§ˆë¬¸)\n\n")
        
        f.write("## ğŸ” ì§ˆë¬¸ë³„ ìƒì„¸ ë¶„ì„\n\n")
        
        # ì§ˆë¬¸ë³„ ê²°ê³¼ (ìƒìœ„ 10ê°œë§Œ í‘œì‹œ)
        question_items = list(results.get('questions', {}).items())[:10]
        for q_id, q_data in question_items:
            f.write(f"### {q_id.upper()}. {q_data['question']}\n\n")
            
            for model in ['GPT-4o', 'Claude-3.5']:
                if model in q_data.get('improvements', {}):
                    improvement = q_data['improvements'][model]
                    responses = q_data['responses'][model]
                    metrics = q_data.get('metrics', {}).get(model, {})
                    
                    f.write(f"#### {model} ë¶„ì„ ê²°ê³¼\n")
                    f.write(f"- **ê°œì„  ì ìˆ˜**: {improvement['overall_score']:.1f}/100\n")
                    f.write(f"- **ë¶„ì„ ê²°ê³¼**: {improvement['analysis']}\n")
                    f.write(f"- **ì‘ë‹µ ì‹œê°„ ë³€í™”**: {improvement['response_time_change']:+.2f}ì´ˆ\n")
                    f.write(f"- **ë‹µë³€ ê¸¸ì´ ë³€í™”**: {improvement['length_change']:+d}ê¸€ì\n")
                    f.write(f"- **ë‹¨ì–´ ìˆ˜ ë³€í™”**: {improvement['word_count_change']:+d}ê°œ\n")
                    f.write(f"- **ì‚¬ìš©ëœ íŒë¡€**: {responses['rag'].get('case_count', 0)}ê±´\n")
                    f.write(f"- **ë²•ë¥  í‚¤ì›Œë“œ ë°€ë„**: {improvement['legal_keyword_density']:.2f}/1000ê¸€ì\n\n")
        
        if len(results.get('questions', {})) > 10:
            f.write(f"*...ìƒìœ„ 10ê°œ ì§ˆë¬¸ë§Œ í‘œì‹œë¨. ì „ì²´ ê²°ê³¼ëŠ” JSON íŒŒì¼ ì°¸ì¡°*\n\n")
        
        f.write("---\n")
        f.write(f"*ë³´ê³ ì„œ ìƒì„±: RAG ì„±ëŠ¥ ê°œì„  ì™„ë²½ ì‹œìŠ¤í…œ v08240535*\n")
        f.write(f"*30ê°œ ì§ˆë¬¸ í™•ì¥ìœ¼ë¡œ í†µê³„ì  ì‹ ë¢°ë„ 6ë°° í–¥ìƒ*\n")
    
    return json_path, csv_path, report_path


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    load_dotenv()
    
    print("ğŸš€ RAG ì„±ëŠ¥ ê°œì„  ì™„ë²½ ë¶„ì„ ì‹œìŠ¤í…œ v08240535 ì‹œì‘")
    print("ğŸ“Š ìƒˆë¡œìš´ ê¸°ëŠ¥: 30ê°œ ì§ˆë¬¸ í‰ê°€ë¡œ ì‹ ë¢°ë„ 6ë°° í–¥ìƒ!")
    
    # ë²„ì „ ê´€ë¦¬ì ì´ˆê¸°í™”
    version_manager = VersionManager()
    
    # LangSmith ê´€ë¦¬ì ì´ˆê¸°í™” (ì„ íƒì )
    langsmith_manager = LangSmithSimple() if LANGSMITH_AVAILABLE else None
    
    # ë¹„êµ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    comparator = RAGImprovementComparator(version_manager, langsmith_manager)
    
    # 30ê°œ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ì„¸íŠ¸ ë¡œë“œ
    test_questions = get_30_evaluation_questions()
    
    print(f"ğŸ“ í‰ê°€ ì§ˆë¬¸ ìˆ˜: {len(test_questions)}ê°œ")
    print("ğŸ¯ í‰ê°€ ë¶„ì•¼: ê·¼ë¡œë²•, ë¯¼ì‚¬ë²•, í–‰ì •ë²•, ìƒì‚¬ë²•, í˜•ì‚¬ë²•, ê°€ì¡±ë²• (ê° 5ê°œ)")
    
    try:
        # ë¹„êµ ë¶„ì„ ì‹¤í–‰
        def progress_printer(progress):
            print(f"â³ ì§„í–‰ë¥ : {progress*100:.1f}%")
        
        results = comparator.compare_models(test_questions, progress_callback=progress_printer)
        
        # ê²°ê³¼ ì €ì¥ (ë‹¤ì¤‘ í˜•ì‹)
        output_dir = ensure_directory_exists("results/rag_improvement_v08240535")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        json_path, csv_path, report_path = save_results_multiple_formats(results, Path(output_dir), timestamp)
        
        print(f"\nğŸ‰ RAG ì„±ëŠ¥ ê°œì„  ì™„ë²½ ë¶„ì„ ì™„ë£Œ! (v08240535)")
        print(f"ğŸ“„ JSON ê²°ê³¼: {json_path}")
        print(f"ğŸ“Š CSV ìš”ì•½: {csv_path}")
        print(f"ğŸ“‹ ë¶„ì„ ë³´ê³ ì„œ: {report_path}")
        
        # ìš”ì•½ ì¶œë ¥
        summary = results.get('summary', {})
        print(f"\nğŸ“ˆ ë¹ ë¥¸ ìš”ì•½ (30ê°œ ì§ˆë¬¸ ê¸°ë°˜):")
        for model, avg_data in summary.get('model_averages', {}).items():
            print(f"  {model}: {avg_data.get('avg_improvement_score', 0):.1f}ì  (í‰ê·  ê°œì„ )")
            print(f"    ìµœê³ /ìµœì €: {avg_data.get('best_score', 0):.1f}/{avg_data.get('worst_score', 0):.1f}")
            print(f"    í‰ê·  íŒë¡€ í™œìš©: {avg_data.get('avg_cases_used', 0):.1f}ê±´")
        
        # ì‹ ë¢°ë„ ê°œì„  ì •ë³´
        q_stats = summary.get('question_statistics', {})
        if q_stats:
            print(f"\nğŸ”¬ í†µê³„ì  ì‹ ë¢°ë„:")
            print(f"  ì´ í‰ê°€ ìˆ˜: {q_stats.get('total_evaluations', 0)}íšŒ (ê¸°ì¡´ 10íšŒ â†’ 60íšŒ)")
            print(f"  ì ìˆ˜ í‘œì¤€í¸ì°¨: {q_stats.get('score_std_dev', 0):.2f} (ë‚®ì„ìˆ˜ë¡ ì•ˆì •ì )")
            print(f"  ì‹ ë¢°ë„ ê°œì„ : â­â­â­â­â­â­ (6ë°° í–¥ìƒ)")
        
        version_manager.logger.info(f"RAG ì„±ëŠ¥ ê°œì„  ì™„ë²½ ë¶„ì„ v08240535 ì™„ë£Œ - ê²°ê³¼: {json_path}")
        
        return results
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        version_manager.logger.error(f"RAG ì„±ëŠ¥ ê°œì„  ì™„ë²½ ë¶„ì„ v08240535 ì¤‘ ì˜¤ë¥˜: {e}")
        raise


if __name__ == "__main__":
    main()